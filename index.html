<!DOCTYPE html>
<html lang="en">
	<head>
		<meta charset="UTF-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1.0" />
		<link
			rel="stylesheet"
			href="//cdnjs.cloudflare.com/ajax/libs/milligram/1.3.0/milligram.css"
		/>
		<title>Speech Doodle</title>
	</head>
	<body>
		<h1>Speech Doodle</h1>
		<div class="canvas-container"></div>
		<canvas class="visualizer" width="500" height="60px"></canvas>

		<button id="record">Record</button>
		<button id="stop">Stop</button>

		<section class="sound-clips"></section>

		<div id="output"></div>
		<article>
			<p></p>
		</article>

		<!-- RECORDER JS -->
		<script src="https://cdn.rawgit.com/mattdiamond/Recorderjs/08e7abd9/dist/recorder.js"></script>
		<!--  -->
		<script>
			// set up basic variables for app

			const record = document.querySelector('#record')
			const stop = document.querySelector('#stop')
			const soundClips = document.querySelector('.sound-clips')
			const canvas = document.querySelector('.visualizer')

			// disable stop button while not recording

			stop.disabled = true

			// visualiser setup - create web audio api context and canvas

			let audioCtx
			const canvasCtx = canvas.getContext('2d')

			//main block for doing the audio recording

			if (navigator.mediaDevices.getUserMedia) {
				console.log('getUserMedia supported.')

				const constraints = { audio: true }
				let chunks = []

				let onSuccess = function(stream) {
					const mediaOptions = {
						audioBitsPerSecond: 16000
					}
					const mediaRecorder = new MediaRecorder(
						stream,
						mediaOptions
					)

					// visualize(stream)

					// MEDIA TYPE AND ENCODING DETAILS
					console.log(mediaRecorder.mimeType)

					record.onclick = function() {
						mediaRecorder.start()
						console.log(mediaRecorder.state)
						console.log('recorder started')
						stop.disabled = false
						record.disabled = true
					}

					stop.onclick = function() {
						mediaRecorder.stop()
						console.log(mediaRecorder.state)
						console.log('recorder stopped')

						stop.disabled = true
						record.disabled = false
					}

					mediaRecorder.onstop = function(e) {
						console.log(
							'data available after MediaRecorder.stop() called.'
						)

						const fileName = 'speech'

						const clipContainer = document.querySelector('article')
						const audio = document.createElement('audio')
						const clipLabel = document.querySelector('article>p')

						audio.setAttribute('controls', '')
						clipLabel.textContent = fileName

						clipContainer.appendChild(audio)

						audio.controls = true
						// AUDIO DATA
						const blob = new Blob(chunks, {
							type: 'audio/ogg; codecs=opus'
						})
						chunks = []
						const audioURL = window.URL.createObjectURL(blob)
						audio.src = audioURL
						console.log('recorder stopped')

						// SEND DATA
						uploadSoundData(blob)
					}

					mediaRecorder.ondataavailable = function(e) {
						chunks.push(e.data)
						console.log(chunks)
					}
				}

				let onError = function(err) {
					console.log('The following error occured: ' + err)
				}

				navigator.mediaDevices
					.getUserMedia(constraints)
					.then(onSuccess, onError)
			} else {
				console.log('getUserMedia not supported on your browser!')
			}

			// FUNTION - UPLOAD AUDIO DATA
			function uploadSoundData(blob) {
				let filename = 'audio' + new Date().toISOString()
				let xhr = new XMLHttpRequest()
				xhr.onload = function(e) {
					if (this.readyState === 4) {
						document.getElementById(
							'output'
						).innerHTML = `<br><br><strong>Result: </strong>${e.target.responseText}`
					}
				}
				let formData = new FormData()
				formData.append('audio_data', blob, filename)
				xhr.open('POST', 'http://localhost:3000/upload-sound', true)
				xhr.send(formData)
			}

			// STREAM VISUALISATION USING HTML CANVAS
			function visualize(stream) {
				if (!audioCtx) {
					audioCtx = new AudioContext()
				}

				const source = audioCtx.createMediaStreamSource(stream)

				const analyser = audioCtx.createAnalyser()
				analyser.fftSize = 2048
				const bufferLength = analyser.frequencyBinCount
				const dataArray = new Uint8Array(bufferLength)

				source.connect(analyser)
				//analyser.connect(audioCtx.destination);

				draw()

				function draw() {
					WIDTH = canvas.width
					HEIGHT = canvas.height

					requestAnimationFrame(draw)

					analyser.getByteTimeDomainData(dataArray)

					canvasCtx.fillStyle = 'rgb(200, 200, 200)'
					canvasCtx.fillRect(0, 0, WIDTH, HEIGHT)

					canvasCtx.lineWidth = 2
					canvasCtx.strokeStyle = 'rgb(0, 0, 0)'

					canvasCtx.beginPath()

					let sliceWidth = (WIDTH * 1.0) / bufferLength
					let x = 0

					for (let i = 0; i < bufferLength; i++) {
						let v = dataArray[i] / 128.0
						let y = (v * HEIGHT) / 2

						if (i === 0) {
							canvasCtx.moveTo(x, y)
						} else {
							canvasCtx.lineTo(x, y)
						}

						x += sliceWidth
					}

					canvasCtx.lineTo(canvas.width, canvas.height / 2)
					canvasCtx.stroke()
				}
			}
		</script>
	</body>
</html>
